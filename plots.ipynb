{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os, json\n",
    "import seaborn as sns\n",
    "\n",
    "def load_and_process_data(benchmark_path: str, results_path: str):\n",
    "    with open(benchmark_path) as f:\n",
    "        benchmark = json.load(f)\n",
    "    with open(results_path) as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    df_benchmark = pd.DataFrame.from_dict(benchmark, orient='index')\n",
    "    \n",
    "    processed_results = {}\n",
    "    for pdb_id, data in results.items():\n",
    "        try:\n",
    "            processed_results[pdb_id] = {\n",
    "                'ba_val': data['ba_val'],\n",
    "                #'kd': data['kd'],\n",
    "                'CC': data['contacts']['CC'],\n",
    "                'CP': data['contacts']['CP'],\n",
    "                'AC': data['contacts']['AC'],\n",
    "                'PP': data['contacts']['PP'],\n",
    "                'AP': data['contacts']['AP'],\n",
    "                'AA': data['contacts']['AA'],\n",
    "                'nis_p': data['nis']['polar'],\n",
    "                'nis_a': data['nis']['aliphatic'],\n",
    "                'nis_c': data['nis']['charged'],\n",
    "                'execution_time': data['execution_time'][\"seconds\"]\n",
    "            }\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Missing data for {pdb_id}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    df_results = pd.DataFrame.from_dict(processed_results, orient='index')\n",
    "    \n",
    "    return df_benchmark, df_results\n",
    "\n",
    "def calculate_correlations(df_benchmark: pd.DataFrame, df_results: pd.DataFrame):\n",
    "    common_pdbs = sorted(set(df_benchmark.index) & set(df_results.index))\n",
    "    print(f\"Common PDB entries: {len(common_pdbs)}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'ba_val': 'Binding Affinity',\n",
    "        'CC': 'Charged-Charged contacts',\n",
    "        'CP': 'Charged-Polar contacts',\n",
    "        'AC': 'Aliphatic-Charged contacts',\n",
    "        'PP': 'Polar-Polar contacts',\n",
    "        'AP': 'Aliphatic-Polar contacts',\n",
    "        'AA': 'Aliphatic-Aliphatic contacts',\n",
    "        'nis_p': 'NIS Polar',\n",
    "        'nis_a': 'NIS Aliphatic',\n",
    "        'nis_c': 'NIS Charged'\n",
    "    }\n",
    "    \n",
    "    correlations = []\n",
    "    for metric in metrics:\n",
    "        if metric in df_benchmark.columns and metric in df_results.columns:\n",
    "            bench_vals = df_benchmark.loc[common_pdbs, metric]\n",
    "            result_vals = df_results.loc[common_pdbs, metric]\n",
    "            pearson = stats.pearsonr(bench_vals, result_vals)\n",
    "            rmse = np.sqrt(np.mean((bench_vals - result_vals) ** 2))\n",
    "            correlations.append({\n",
    "                'Metric': metrics[metric],\n",
    "                'Pearson r': pearson[0],\n",
    "                'p-value': pearson[1],\n",
    "                'RMSE': rmse\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(correlations)\n",
    "\n",
    "def plot_correlations(df_benchmark: pd.DataFrame, df_results: pd.DataFrame, output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    common_pdbs = sorted(set(df_benchmark.index) & set(df_results.index))\n",
    "    \n",
    "    metrics = {\n",
    "        'ba_val': 'Binding Affinity',\n",
    "        'CC': 'Charged-Charged contacts',\n",
    "        'CP': 'Charged-Polar contacts',\n",
    "        'AC': 'Aliphatic-Charged contacts',\n",
    "        'PP': 'Polar-Polar contacts',\n",
    "        'AP': 'Aliphatic-Polar contacts',\n",
    "        'AA': 'Aliphatic-Aliphatic contacts',\n",
    "        'nis_p': 'NIS Polar',\n",
    "        'nis_a': 'NIS Aliphatic',\n",
    "        'nis_c': 'NIS Charged'\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (metric, title) in enumerate(metrics.items()):\n",
    "        if i < len(axes):\n",
    "            bench_vals = df_benchmark.loc[common_pdbs, metric]\n",
    "            result_vals = df_results.loc[common_pdbs, metric]\n",
    "            \n",
    "            pearson = stats.pearsonr(bench_vals, result_vals)[0]\n",
    "            \n",
    "            ax = axes[i]\n",
    "            ax.scatter(bench_vals, result_vals, alpha=0.6)\n",
    "            ax.plot([min(bench_vals), max(bench_vals)], \n",
    "                   [min(bench_vals), max(bench_vals)], 'r--')\n",
    "            \n",
    "            ax.set_xlabel('Prodigy ORG')\n",
    "            ax.set_ylabel('Prodigy JAX')\n",
    "            ax.set_title(f'{title}\\nr = {pearson:.3f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def add_sequence_lengths(df: pd.DataFrame, pdb_folder: str):\n",
    "    lengths = {}\n",
    "    for pdb_id in df.index:\n",
    "        try:\n",
    "            pdb_file = os.path.join(pdb_folder, f\"{pdb_id}.pdb\")\n",
    "            if os.path.exists(pdb_file):\n",
    "                with open(pdb_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    # Count ATOM lines for chain A and B\n",
    "                    chain_a = sum(1 for line in lines if line.startswith('ATOM') and line[21] == 'A')\n",
    "                    chain_b = sum(1 for line in lines if line.startswith('ATOM') and line[21] == 'B')\n",
    "                    # Divide by typical number of atoms per residue (usually around 8-10)\n",
    "                    lengths[pdb_id] = {\n",
    "                        'chain_a_length': chain_a // 8,\n",
    "                        'chain_b_length': chain_b // 8\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdb_id}: {e}\")\n",
    "            \n",
    "    # Add to DataFrame\n",
    "    length_df = pd.DataFrame.from_dict(lengths, orient='index')\n",
    "    return pd.concat([df, length_df], axis=1)\n",
    "\n",
    "def compare_sasa_results(gpu_dir: str, cpu_dir: str):\n",
    "    gpu_path = Path(gpu_dir)\n",
    "    cpu_path = Path(cpu_dir)\n",
    "    all_comparisons = []\n",
    "    all_sasa_values = []\n",
    "    \n",
    "    for protein_dir in gpu_path.glob(\"*\"):\n",
    "        print(\"\")\n",
    "        if not protein_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        protein_name = protein_dir.name\n",
    "        gpu_csv = list(protein_dir.rglob(\"*.csv\"))\n",
    "        cpu_csv = list((cpu_path / protein_name).rglob(\"*.csv\"))\n",
    "        \n",
    "        if not gpu_csv or not cpu_csv:\n",
    "            continue\n",
    "        \n",
    "        gpu_data = pd.read_csv(gpu_csv[0])\n",
    "        gpu_data.resid = gpu_data.resid.astype(int)\n",
    "        gpu_data = gpu_data.sort_values(['chain', \"resname\", 'resid', 'atom'])\n",
    "        cpu_data = pd.read_csv(cpu_csv[0])\n",
    "        cpu_data.resid = cpu_data.resid.astype(int)\n",
    "        cpu_data = cpu_data.sort_values(['chain', \"resname\", 'resid', 'atom'])\n",
    "\n",
    "        if len(gpu_data) != len(cpu_data):\n",
    "            print(f\"Length mismatch in {protein_name}: GPU={len(gpu_data)}, CPU={len(cpu_data)}\")\n",
    "            continue\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "          'sasa_cpu': cpu_data['sasa'].values,\n",
    "          'sasa_gpu': gpu_data['sasa'].values,\n",
    "          'diff': abs(cpu_data['sasa'].values - gpu_data['sasa'].values),\n",
    "          'protein': protein_name,\n",
    "          'chain_gpu': gpu_data['chain'].values,\n",
    "          'resname_gpu': gpu_data['resname'].values, \n",
    "          'resid_gpu': gpu_data['resid'].values,\n",
    "          'atom_gpu': gpu_data['atom'].values,\n",
    "          'chain_cpu': cpu_data['chain'].values,\n",
    "          'resname_cpu': cpu_data['resname'].values,\n",
    "          'resid_cpu': cpu_data['resid'].values, \n",
    "          'atom_cpu': cpu_data['atom'].values\n",
    "        })\n",
    "        \n",
    "        all_sasa_values.append(comparison)\n",
    "        \n",
    "        rmse = np.sqrt(np.mean(comparison['diff']**2))\n",
    "        correlation = stats.pearsonr(comparison['sasa_cpu'], comparison['sasa_gpu'])[0]\n",
    "        \n",
    "        all_comparisons.append({\n",
    "            'protein': protein_name,\n",
    "            'rmse': rmse,\n",
    "            'correlation': correlation,\n",
    "            'mean_diff': comparison['diff'].mean(),\n",
    "            'max_diff': comparison['diff'].max(),\n",
    "            'num_atoms': len(comparison),\n",
    "            'num_nonzero': len(comparison[comparison['sasa_gpu'] > 0])\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(all_comparisons)\n",
    "    all_sasa_df = pd.concat(all_sasa_values)\n",
    "    \n",
    "    # Add high_rmse column to all_sasa_df\n",
    "    high_rmse_proteins = set(summary_df[summary_df['rmse'] > 2]['protein'])\n",
    "    all_sasa_df['high_rmse'] = all_sasa_df['protein'].isin(high_rmse_proteins)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # RMSE scatter plot\n",
    "    ax1.scatter(summary_df['num_atoms'], summary_df['rmse'], alpha=0.6)\n",
    "    for i, txt in enumerate(summary_df['protein']):\n",
    "        if summary_df['rmse'].iloc[i] > 2:\n",
    "            ax1.annotate(txt, (summary_df['num_atoms'].iloc[i], summary_df['rmse'].iloc[i]))\n",
    "    ax1.set_xlabel('Number of Atoms')\n",
    "    ax1.set_ylabel('RMSE (Å²)')\n",
    "    ax1.set_title('RMSE GPU and CPU vs Structure Size')\n",
    "    \n",
    "    # SASA values comparison with color coding\n",
    "    normal_points = all_sasa_df[~all_sasa_df['high_rmse']]\n",
    "    high_rmse_points = all_sasa_df[all_sasa_df['high_rmse']]\n",
    "    \n",
    "    ax2.scatter(normal_points['sasa_cpu'], normal_points['sasa_gpu'], alpha=0.1, color='blue')\n",
    "    \n",
    "    max_val = max(all_sasa_df['sasa_cpu'].max(), all_sasa_df['sasa_gpu'].max())\n",
    "    ax2.plot([0, max_val], [0, max_val], 'k--')\n",
    "    ax2.set_xlabel('CPU SASA (Å²)')\n",
    "    ax2.set_ylabel('GPU SASA (Å²)')\n",
    "    ax2.set_title('CPU vs GPU SASA Values')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sasa_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return summary_df, all_sasa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = \"./benchmark_af/dataset.json\"\n",
    "results = \"./benchmark_af/20250126_140901_gpu/combined_results.json\"\n",
    "output = \"output_comp\"\n",
    "dataset = \"./benchmark_af/PRODIGYdataset/\" # make sure you have the dataset\n",
    "os.makedirs(output, exist_ok=True)\n",
    "df_benchmark, df_results = load_and_process_data(benchmark, results)\n",
    "correlations = calculate_correlations(df_benchmark, df_results)\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "print(correlations.to_string(index=False))\n",
    "correlations.to_csv(f'{output}/correlations.csv', index=False)\n",
    "plot_correlations(df_benchmark, df_results, output)\n",
    "\n",
    "# Save processed DataFrames\n",
    "df_benchmark.to_csv(f'{output}/benchmark_processed.csv')\n",
    "df_results.to_csv(f'{output}/results_processed.csv')\n",
    "\n",
    "df = add_sequence_lengths(df_results, dataset)\n",
    "df['total_length'] = df['chain_a_length'] + df['chain_b_length']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['total_length'], df['execution_time'])\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['total_length'], df['execution_time'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df['total_length'], p(df['total_length']), \"r--\", alpha=0.8)\n",
    "\n",
    "# Calculate correlation\n",
    "corr = df['total_length'].corr(df['execution_time'])\n",
    "\n",
    "plt.xlabel('Total Sequence Length (residues)')\n",
    "plt.ylabel('Execution Time (s)')\n",
    "plt.title(f'Execution Time vs Sequence Length\\nCorrelation: {corr:.3f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "summary_df, all_sasa_df = compare_sasa_results(\"./benchmark_af/20250126_140901_gpu\", \"./benchmark_af/20250127_160612_cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio_lib.custom_prodigy import predict_binding_affinity\n",
    "\n",
    "predict_binding_affinity(\"/Users/alessio/Documents/Repos/bio_lib/3bzd.pdb\", save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import bio_lib.common.residue_constants as residue_constants\n",
    "import bio_lib.common.protein as Protein\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "def create_test_data(target: Protein, binder: Protein):\n",
    "    \"\"\"Generate random SASA data matching protein dimensions.\"\"\"\n",
    "    # Get residue counts\n",
    "    target_res = target.aatype.shape[0]\n",
    "    binder_res = binder.aatype.shape[0]\n",
    "    \n",
    "    # Create random SASA values (0-1 range)\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    \n",
    "    # Complex SASA (per-atom)\n",
    "    complex_shape = (target_res + binder_res) * 37\n",
    "    complex_sasa = jax.random.uniform(rng, (complex_shape,)) * 0.5  # Most values < 0.5\n",
    "    \n",
    "    # Relative SASA (per-residue)\n",
    "    relative_shape = target_res + binder_res\n",
    "    relative_sasa = jax.random.uniform(rng, (relative_shape,))\n",
    "    \n",
    "    return complex_sasa, relative_sasa\n",
    "\n",
    "\n",
    "def convert_sasa_to_array(\n",
    "    complex_sasa: jnp.ndarray,\n",
    "    relative_sasa: jnp.ndarray,\n",
    "    target: Protein,\n",
    "    binder: Protein,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Create structured array with columns: \n",
    "    (chain, resn, resi, atom, atom_sasa, residue_sasa)\"\"\"\n",
    "    atoms_per_res = 37\n",
    "    \n",
    "    # Helper to process each chain\n",
    "    def process_chain(protein: Protein, chain_id: int):\n",
    "        num_res = protein.aatype.shape[0]\n",
    "        res_indices = jnp.arange(1, num_res + 1)\n",
    "        \n",
    "        # Create full grid of [res_idx, atom_idx]\n",
    "        res_idx_grid = jnp.repeat(res_indices[:, None], atoms_per_res, axis=1)\n",
    "        atom_idx_grid = jnp.tile(jnp.arange(atoms_per_res), (num_res, 1))\n",
    "        chain_grid = jnp.full_like(res_idx_grid, chain_id)\n",
    "        aatype_grid = jnp.repeat(protein.aatype[:, None], atoms_per_res, axis=1)\n",
    "        \n",
    "        return (chain_grid, res_idx_grid, aatype_grid, atom_idx_grid)\n",
    "\n",
    "    # Process target and binder\n",
    "    target_chain, target_res_idx, target_aatype, target_atom_idx = process_chain(target, 0)\n",
    "    binder_chain, binder_res_idx, binder_aatype, binder_atom_idx = process_chain(binder, 1)\n",
    "\n",
    "    # Combine and flatten all atom data\n",
    "    all_chains = jnp.concatenate([target_chain, binder_chain]).ravel()\n",
    "    all_res_indices = jnp.concatenate([target_res_idx, binder_res_idx]).ravel()\n",
    "    all_aatypes = jnp.concatenate([target_aatype, binder_aatype]).ravel()\n",
    "    all_atom_indices = jnp.concatenate([target_atom_idx, binder_atom_idx]).ravel()\n",
    "    all_sasa = complex_sasa.ravel()\n",
    "\n",
    "    # Filter valid entries (SASA > 0)\n",
    "    mask = all_sasa > 0\n",
    "    filtered_data = (\n",
    "        all_chains[mask],\n",
    "        all_res_indices[mask],\n",
    "        all_aatypes[mask],\n",
    "        all_atom_indices[mask],\n",
    "        all_sasa[mask]\n",
    "    )\n",
    "\n",
    "    # Process relative SASA\n",
    "    target_rel = relative_sasa[:len(target.aatype)]\n",
    "    binder_rel = relative_sasa[len(target.aatype):]\n",
    "    \n",
    "    rel_chains = jnp.concatenate([jnp.zeros_like(target_rel), jnp.ones_like(binder_rel)])\n",
    "    rel_res_indices = jnp.concatenate([jnp.arange(1, len(target_rel)+1), jnp.arange(1, len(binder_rel)+1)])\n",
    "    rel_aatypes = jnp.concatenate([target.aatype, binder.aatype])\n",
    "    rel_mask = jnp.concatenate([target_rel > 0, binder_rel > 0])\n",
    "    \n",
    "    rel_filtered = (\n",
    "        rel_chains[rel_mask],\n",
    "        rel_res_indices[rel_mask],\n",
    "        rel_aatypes[rel_mask],\n",
    "        jnp.concatenate([target_rel, binder_rel])[rel_mask]\n",
    "    )\n",
    "\n",
    "    # Get existing filtered data from previous processing\n",
    "    (chain_np, res_np, aatype_np, atom_np, sasa_np) = map(np.array, filtered_data)\n",
    "    (rel_chain_np, rel_res_np, rel_aatype_np, rel_sasa_np) = map(np.array, rel_filtered)\n",
    "\n",
    "    # Create residue keys for mapping\n",
    "    residue_keys = np.core.defchararray.add(\n",
    "        np.core.defchararray.add(\n",
    "            rel_chain_np.astype('U1'), \n",
    "            rel_res_np.astype('U4')\n",
    "        ),\n",
    "        rel_aatype_np.astype('U3')\n",
    "    )\n",
    "    \n",
    "    # Create atom keys with same format, not needed\n",
    "    atom_res_keys = np.core.defchararray.add(\n",
    "        np.core.defchararray.add(\n",
    "            chain_np.astype('U1'), \n",
    "            res_np.astype('U4')\n",
    "        ),\n",
    "        aatype_np.astype('U3')\n",
    "    )\n",
    "\n",
    "    # Find indices for residue SASA lookup\n",
    "    _, rev_idx = np.unique(residue_keys, return_inverse=True)\n",
    "    residue_sasa_expanded = rel_sasa_np[rev_idx]\n",
    "\n",
    "    # Create structured dtype\n",
    "    dtype = [\n",
    "        ('chain', 'U1'),\n",
    "        ('resn', 'U3'),\n",
    "        ('resi', int),\n",
    "        ('atom', 'U4'),\n",
    "        ('atom_sasa', float),\n",
    "        ('residue_sasa', float)\n",
    "    ]\n",
    "    \n",
    "    # Convert residue types to names\n",
    "    restypes = residue_constants.restypes + ['X']\n",
    "    restype_1to3 = residue_constants.restype_1to3\n",
    "    res_names = np.array([restype_1to3.get(restypes[aa], 'UNK') for aa in aatype_np])\n",
    "    \n",
    "    # Convert chain indices to letters\n",
    "    chain_letters = np.where(chain_np == 0, 'A', 'B')\n",
    "    \n",
    "    # Get atom names\n",
    "    atom_names = np.array(residue_constants.atom_types)[atom_np]\n",
    "\n",
    "    # Build final array\n",
    "    return np.array(\n",
    "        list(zip(\n",
    "            chain_letters,\n",
    "            res_names,\n",
    "            res_np.astype(int),\n",
    "            atom_names,\n",
    "            sasa_np,\n",
    "            residue_sasa_expanded\n",
    "        )),\n",
    "        dtype=dtype\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio_lib.custom_prodigy_jax import load_pdb_to_af\n",
    "\n",
    "pdb_path = \"/Users/alessio/Documents/Repos/dr_sasa_python/data/PRODIGYdataset/2MTA.pdb\"\n",
    "target, binder = load_pdb_to_af(pdb_path, \"A\", \"B\")\n",
    "complex_sasa, relative_sasa = create_test_data(target, binder)\n",
    "# Run conversion\n",
    "sasa_dict = convert_sasa_to_array(\n",
    "    complex_sasa=complex_sasa,\n",
    "    relative_sasa=relative_sasa,\n",
    "    target=target,\n",
    "    binder=binder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_sasa_to_array_simple"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_lib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
