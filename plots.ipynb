{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import os, json\n",
    "import seaborn as sns\n",
    "\n",
    "def load_and_process_data(benchmark_path: str, results_path: str):\n",
    "    with open(benchmark_path) as f:\n",
    "        benchmark = json.load(f)\n",
    "    with open(results_path) as f:\n",
    "        results = json.load(f)\n",
    "        \n",
    "    df_benchmark = pd.DataFrame.from_dict(benchmark, orient='index')\n",
    "    \n",
    "    processed_results = {}\n",
    "    for pdb_id, data in results.items():\n",
    "        try:\n",
    "            processed_results[pdb_id] = {\n",
    "                'ba_val': data['ba_val'],\n",
    "                #'kd': data['kd'],\n",
    "                'CC': data['contacts']['CC'],\n",
    "                'CP': data['contacts']['CP'],\n",
    "                'AC': data['contacts']['AC'],\n",
    "                'PP': data['contacts']['PP'],\n",
    "                'AP': data['contacts']['AP'],\n",
    "                'AA': data['contacts']['AA'],\n",
    "                'nis_p': data['nis']['polar'],\n",
    "                'nis_a': data['nis']['aliphatic'],\n",
    "                'nis_c': data['nis']['charged'],\n",
    "                'execution_time': data['execution_time'][\"seconds\"]\n",
    "            }\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Missing data for {pdb_id}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    df_results = pd.DataFrame.from_dict(processed_results, orient='index')\n",
    "    \n",
    "    return df_benchmark, df_results\n",
    "\n",
    "def calculate_correlations(df_benchmark: pd.DataFrame, df_results: pd.DataFrame):\n",
    "    common_pdbs = sorted(set(df_benchmark.index) & set(df_results.index))\n",
    "    print(f\"Common PDB entries: {len(common_pdbs)}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'ba_val': 'Binding Affinity',\n",
    "        'CC': 'Charged-Charged contacts',\n",
    "        'CP': 'Charged-Polar contacts',\n",
    "        'AC': 'Aliphatic-Charged contacts',\n",
    "        'PP': 'Polar-Polar contacts',\n",
    "        'AP': 'Aliphatic-Polar contacts',\n",
    "        'AA': 'Aliphatic-Aliphatic contacts',\n",
    "        'nis_p': 'NIS Polar',\n",
    "        'nis_a': 'NIS Aliphatic',\n",
    "        'nis_c': 'NIS Charged'\n",
    "    }\n",
    "    \n",
    "    correlations = []\n",
    "    for metric in metrics:\n",
    "        if metric in df_benchmark.columns and metric in df_results.columns:\n",
    "            bench_vals = df_benchmark.loc[common_pdbs, metric]\n",
    "            result_vals = df_results.loc[common_pdbs, metric]\n",
    "            pearson = stats.pearsonr(bench_vals, result_vals)\n",
    "            rmse = np.sqrt(np.mean((bench_vals - result_vals) ** 2))\n",
    "            correlations.append({\n",
    "                'Metric': metrics[metric],\n",
    "                'Pearson r': pearson[0],\n",
    "                'p-value': pearson[1],\n",
    "                'RMSE': rmse\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(correlations)\n",
    "\n",
    "def plot_correlations(df_benchmark: pd.DataFrame, df_results: pd.DataFrame, output_dir: str):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    common_pdbs = sorted(set(df_benchmark.index) & set(df_results.index))\n",
    "    \n",
    "    metrics = {\n",
    "        'ba_val': 'Binding Affinity',\n",
    "        'CC': 'Charged-Charged contacts',\n",
    "        'CP': 'Charged-Polar contacts',\n",
    "        'AC': 'Aliphatic-Charged contacts',\n",
    "        'PP': 'Polar-Polar contacts',\n",
    "        'AP': 'Aliphatic-Polar contacts',\n",
    "        'AA': 'Aliphatic-Aliphatic contacts',\n",
    "        'nis_p': 'NIS Polar',\n",
    "        'nis_a': 'NIS Aliphatic',\n",
    "        'nis_c': 'NIS Charged'\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (metric, title) in enumerate(metrics.items()):\n",
    "        if i < len(axes):\n",
    "            bench_vals = df_benchmark.loc[common_pdbs, metric]\n",
    "            result_vals = df_results.loc[common_pdbs, metric]\n",
    "            \n",
    "            pearson = stats.pearsonr(bench_vals, result_vals)[0]\n",
    "            \n",
    "            ax = axes[i]\n",
    "            ax.scatter(bench_vals, result_vals, alpha=0.6)\n",
    "            ax.plot([min(bench_vals), max(bench_vals)], \n",
    "                   [min(bench_vals), max(bench_vals)], 'r--')\n",
    "            \n",
    "            ax.set_xlabel('Prodigy ORG')\n",
    "            ax.set_ylabel('Prodigy JAX')\n",
    "            ax.set_title(f'{title}\\nr = {pearson:.3f}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def add_sequence_lengths(df: pd.DataFrame, pdb_folder: str):\n",
    "    lengths = {}\n",
    "    for pdb_id in df.index:\n",
    "        try:\n",
    "            pdb_file = os.path.join(pdb_folder, f\"{pdb_id}.pdb\")\n",
    "            if os.path.exists(pdb_file):\n",
    "                with open(pdb_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    # Count ATOM lines for chain A and B\n",
    "                    chain_a = sum(1 for line in lines if line.startswith('ATOM') and line[21] == 'A')\n",
    "                    chain_b = sum(1 for line in lines if line.startswith('ATOM') and line[21] == 'B')\n",
    "                    # Divide by typical number of atoms per residue (usually around 8-10)\n",
    "                    lengths[pdb_id] = {\n",
    "                        'chain_a_length': chain_a // 8,\n",
    "                        'chain_b_length': chain_b // 8\n",
    "                    }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {pdb_id}: {e}\")\n",
    "            \n",
    "    # Add to DataFrame\n",
    "    length_df = pd.DataFrame.from_dict(lengths, orient='index')\n",
    "    return pd.concat([df, length_df], axis=1)\n",
    "\n",
    "def compare_sasa_results(gpu_dir: str, cpu_dir: str):\n",
    "    gpu_path = Path(gpu_dir)\n",
    "    cpu_path = Path(cpu_dir)\n",
    "    all_comparisons = []\n",
    "    all_sasa_values = []\n",
    "    \n",
    "    for protein_dir in gpu_path.glob(\"*\"):\n",
    "        print(\"\")\n",
    "        if not protein_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        protein_name = protein_dir.name\n",
    "        gpu_csv = list(protein_dir.rglob(\"*.csv\"))\n",
    "        cpu_csv = list((cpu_path / protein_name).rglob(\"*.csv\"))\n",
    "        \n",
    "        if not gpu_csv or not cpu_csv:\n",
    "            continue\n",
    "        \n",
    "        gpu_data = pd.read_csv(gpu_csv[0])\n",
    "        gpu_data.resid = gpu_data.resid.astype(int)\n",
    "        gpu_data = gpu_data.sort_values(['chain', \"resname\", 'resid', 'atom'])\n",
    "        cpu_data = pd.read_csv(cpu_csv[0])\n",
    "        cpu_data.resid = cpu_data.resid.astype(int)\n",
    "        cpu_data = cpu_data.sort_values(['chain', \"resname\", 'resid', 'atom'])\n",
    "\n",
    "        if len(gpu_data) != len(cpu_data):\n",
    "            print(f\"Length mismatch in {protein_name}: GPU={len(gpu_data)}, CPU={len(cpu_data)}\")\n",
    "            continue\n",
    "        \n",
    "        comparison = pd.DataFrame({\n",
    "          'sasa_cpu': cpu_data['sasa'].values,\n",
    "          'sasa_gpu': gpu_data['sasa'].values,\n",
    "          'diff': abs(cpu_data['sasa'].values - gpu_data['sasa'].values),\n",
    "          'protein': protein_name,\n",
    "          'chain_gpu': gpu_data['chain'].values,\n",
    "          'resname_gpu': gpu_data['resname'].values, \n",
    "          'resid_gpu': gpu_data['resid'].values,\n",
    "          'atom_gpu': gpu_data['atom'].values,\n",
    "          'chain_cpu': cpu_data['chain'].values,\n",
    "          'resname_cpu': cpu_data['resname'].values,\n",
    "          'resid_cpu': cpu_data['resid'].values, \n",
    "          'atom_cpu': cpu_data['atom'].values\n",
    "        })\n",
    "        \n",
    "        all_sasa_values.append(comparison)\n",
    "        \n",
    "        rmse = np.sqrt(np.mean(comparison['diff']**2))\n",
    "        correlation = stats.pearsonr(comparison['sasa_cpu'], comparison['sasa_gpu'])[0]\n",
    "        \n",
    "        all_comparisons.append({\n",
    "            'protein': protein_name,\n",
    "            'rmse': rmse,\n",
    "            'correlation': correlation,\n",
    "            'mean_diff': comparison['diff'].mean(),\n",
    "            'max_diff': comparison['diff'].max(),\n",
    "            'num_atoms': len(comparison),\n",
    "            'num_nonzero': len(comparison[comparison['sasa_gpu'] > 0])\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(all_comparisons)\n",
    "    all_sasa_df = pd.concat(all_sasa_values)\n",
    "    \n",
    "    # Add high_rmse column to all_sasa_df\n",
    "    high_rmse_proteins = set(summary_df[summary_df['rmse'] > 2]['protein'])\n",
    "    all_sasa_df['high_rmse'] = all_sasa_df['protein'].isin(high_rmse_proteins)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # RMSE scatter plot\n",
    "    ax1.scatter(summary_df['num_atoms'], summary_df['rmse'], alpha=0.6)\n",
    "    for i, txt in enumerate(summary_df['protein']):\n",
    "        if summary_df['rmse'].iloc[i] > 2:\n",
    "            ax1.annotate(txt, (summary_df['num_atoms'].iloc[i], summary_df['rmse'].iloc[i]))\n",
    "    ax1.set_xlabel('Number of Atoms')\n",
    "    ax1.set_ylabel('RMSE (Å²)')\n",
    "    ax1.set_title('RMSE GPU and CPU vs Structure Size')\n",
    "    \n",
    "    # SASA values comparison with color coding\n",
    "    normal_points = all_sasa_df[~all_sasa_df['high_rmse']]\n",
    "    high_rmse_points = all_sasa_df[all_sasa_df['high_rmse']]\n",
    "    \n",
    "    ax2.scatter(normal_points['sasa_cpu'], normal_points['sasa_gpu'], alpha=0.1, color='blue')\n",
    "    \n",
    "    max_val = max(all_sasa_df['sasa_cpu'].max(), all_sasa_df['sasa_gpu'].max())\n",
    "    ax2.plot([0, max_val], [0, max_val], 'k--')\n",
    "    ax2.set_xlabel('CPU SASA (Å²)')\n",
    "    ax2.set_ylabel('GPU SASA (Å²)')\n",
    "    ax2.set_title('CPU vs GPU SASA Values')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sasa_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return summary_df, all_sasa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = \"./benchmark_af/dataset.json\"\n",
    "results = \"./benchmark_af/20250126_140901_gpu/combined_results.json\"\n",
    "output = \"output_comp\"\n",
    "dataset = \"./benchmark_af/PRODIGYdataset/\" # make sure you have the dataset\n",
    "os.makedirs(output, exist_ok=True)\n",
    "df_benchmark, df_results = load_and_process_data(benchmark, results)\n",
    "correlations = calculate_correlations(df_benchmark, df_results)\n",
    "print(\"\\nCorrelation Analysis:\")\n",
    "print(correlations.to_string(index=False))\n",
    "correlations.to_csv(f'{output}/correlations.csv', index=False)\n",
    "plot_correlations(df_benchmark, df_results, output)\n",
    "\n",
    "# Save processed DataFrames\n",
    "df_benchmark.to_csv(f'{output}/benchmark_processed.csv')\n",
    "df_results.to_csv(f'{output}/results_processed.csv')\n",
    "\n",
    "df = add_sequence_lengths(df_results, dataset)\n",
    "df['total_length'] = df['chain_a_length'] + df['chain_b_length']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['total_length'], df['execution_time'])\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['total_length'], df['execution_time'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df['total_length'], p(df['total_length']), \"r--\", alpha=0.8)\n",
    "\n",
    "# Calculate correlation\n",
    "corr = df['total_length'].corr(df['execution_time'])\n",
    "\n",
    "plt.xlabel('Total Sequence Length (residues)')\n",
    "plt.ylabel('Execution Time (s)')\n",
    "plt.title(f'Execution Time vs Sequence Length\\nCorrelation: {corr:.3f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "summary_df, all_sasa_df = compare_sasa_results(\"./benchmark_af/20250126_140901_gpu\", \"./benchmark_af/20250127_160612_cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_binding_affinity_jax(\"/Users/alessio/Documents/Repos/dr_sasa_python/data/PRODIGYdataset/1ACB.pdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = res.sasa_data[\"atom_sasa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "o = pd.read_csv(\"/Users/alessio/Documents/Repos/bio_lib/1ACB_sasa_data.csv\").sasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(o -r).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bio_lib.custom_prodigy import predict_binding_affinity\n",
    "\n",
    "res = predict_binding_affinity(\"/Users/alessio/Documents/Repos/dr_sasa_python/data/PRODIGYdataset/1ACB.pdb\", selection=\"A,B\", output_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_performance_log(log_text):\n",
    "    # Split the log by 'Processing'\n",
    "    pdb_sections = log_text.split('Processing')[1:]\n",
    "    \n",
    "    # Lists to store parsed data\n",
    "    data = []\n",
    "    \n",
    "    for section in pdb_sections:\n",
    "        # Extract PDB filename\n",
    "        pdb_match = re.search(r'(.+\\.pdb)\\nNumber of atoms: (\\d+)', section)\n",
    "        if not pdb_match:\n",
    "            continue\n",
    "        \n",
    "        pdb_file = pdb_match.group(1).strip()\n",
    "        num_atoms = int(pdb_match.group(2))\n",
    "        \n",
    "        # Find all block size measurements\n",
    "        block_matches = re.finditer(r'Block size (\\d+):\\n  Run times: \\[(.*?)\\]\\n  Mean: ([\\d.]+)s, Median: ([\\d.]+)s, Std: ([\\d.]+)s', section)\n",
    "        \n",
    "        for block_match in block_matches:\n",
    "            block_size = int(block_match.group(1))\n",
    "            \n",
    "            # Parse run times\n",
    "            run_times = [float(t.strip().strip(\"'s\")) for t in block_match.group(2).split(',')]\n",
    "            \n",
    "            # Ensure we have exactly 3 run times\n",
    "            while len(run_times) < 3:\n",
    "                run_times.append(None)\n",
    "            \n",
    "            # Parse statistics\n",
    "            mean_time = float(block_match.group(3))\n",
    "            median_time = float(block_match.group(4))\n",
    "            std_time = float(block_match.group(5))\n",
    "            \n",
    "            data.append({\n",
    "                'PDB_File': Path(pdb_file).stem,\n",
    "                'Number_of_Atoms': num_atoms,\n",
    "                'Block_Size': block_size,\n",
    "                'Run1_Time': run_times[0],\n",
    "                'Run2_Time': run_times[1],\n",
    "                'Run3_Time': run_times[2],\n",
    "                'Mean_Time': mean_time,\n",
    "                'Median_Time': median_time,\n",
    "                'Std_Time': std_time\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Read the log file content\n",
    "with open('paste.txt', 'r') as file:\n",
    "    log_text = file.read()\n",
    "\n",
    "# Parse the log\n",
    "df = parse_performance_log(log_text)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "print(\"\\nDataFrame Info:\")\n",
    "df.info()\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df.to_csv('pdb_performance_data.csv', index=False)\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.groupby('PDB_File').agg({\n",
    "    'Number_of_Atoms': 'first',\n",
    "    'Mean_Time': ['mean', 'min', 'max'],\n",
    "    'Block_Size': ['min', 'max']\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "# Group by PDB file and take the first row for each\n",
    "grouped_data = df.groupby('PDB_File').first().reset_index()\n",
    "\n",
    "# Exponential fitting function for Mean Time (increasing)\n",
    "def exp_func_time(x, a, b, c):\n",
    "    return a * np.exp(b * x) + c\n",
    "\n",
    "# Exponential decay function for Block Size\n",
    "def exp_decay_func(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "# Prepare data for fitting\n",
    "x = grouped_data['Number_of_Atoms']\n",
    "y_time = grouped_data['Mean_Time']\n",
    "y_blocksize = grouped_data['Block_Size']\n",
    "\n",
    "# Perform curve fitting for Mean Time\n",
    "popt_time, _ = curve_fit(exp_func_time, x, y_time, p0=[0.001, 0.0001, 1])\n",
    "\n",
    "# Perform curve fitting for Block Size (using decay function)\n",
    "popt_blocksize, _ = curve_fit(exp_decay_func, x, y_blocksize, p0=[100, 0.0001, 1])\n",
    "\n",
    "# Generate points for the fitted curves\n",
    "x_fit = np.linspace(x.min(), x.max(), 100)\n",
    "y_fit_time = exp_func_time(x_fit, *popt_time)\n",
    "y_fit_blocksize = exp_decay_func(x_fit, *popt_blocksize)\n",
    "\n",
    "# Create the plot with two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# First y-axis - Mean Time\n",
    "color1 = 'blue'\n",
    "ax1.set_xlabel('Number of Atoms')\n",
    "ax1.set_ylabel('Mean Time (s)', color=color1)\n",
    "ax1.errorbar(grouped_data['Number_of_Atoms'], grouped_data['Mean_Time'], \n",
    "             yerr=grouped_data['Std_Time'], \n",
    "             fmt='o', \n",
    "             capsize=5, \n",
    "             color=color1, \n",
    "             alpha=0.7, \n",
    "             label='Mean Time with Std Dev')\n",
    "ax1.plot(x_fit, y_fit_time, color=color1, linestyle='--', \n",
    "         label=f'Time Fit: {popt_time[0]:.4e} * exp({popt_time[1]:.4e} * x) + {popt_time[2]:.4f}')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "# Calculate R-squared for Mean Time\n",
    "residuals_time = y_time - exp_func_time(x, *popt_time)\n",
    "ss_res_time = np.sum(residuals_time**2)\n",
    "ss_tot_time = np.sum((y_time - np.mean(y_time))**2)\n",
    "r_squared_time = 1 - (ss_res_time / ss_tot_time)\n",
    "\n",
    "# Second y-axis - Block Size\n",
    "ax2 = ax1.twinx()\n",
    "color2 = 'red'\n",
    "ax2.set_ylabel('Block Size', color=color2)\n",
    "ax2.scatter(grouped_data['Number_of_Atoms'], grouped_data['Block_Size'], \n",
    "            color=color2, alpha=0.7, label='Block Size')\n",
    "ax2.plot(x_fit, y_fit_blocksize, color=color2, linestyle='--', \n",
    "         label=f'Block Size Fit: {popt_blocksize[0]:.4e} * exp(-{popt_blocksize[1]:.4e} * x) + {popt_blocksize[2]:.4f}')\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "# Calculate R-squared for Block Size\n",
    "residuals_blocksize = y_blocksize - exp_decay_func(x, *popt_blocksize)\n",
    "ss_res_blocksize = np.sum(residuals_blocksize**2)\n",
    "ss_tot_blocksize = np.sum((y_blocksize - np.mean(y_blocksize))**2)\n",
    "r_squared_blocksize = 1 - (ss_res_blocksize / ss_tot_blocksize)\n",
    "\n",
    "# Title and layout\n",
    "plt.title('Number of Atoms vs Mean Time and Block Size')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='best')\n",
    "\n",
    "# Add R-squared annotations\n",
    "ax1.annotate(f'R² (Time) = {r_squared_time:.4f}', \n",
    "             xy=(0.05, 0.95), \n",
    "             xycoords='axes fraction', \n",
    "             fontsize=10, \n",
    "             color=color1,\n",
    "             verticalalignment='top')\n",
    "ax1.annotate(f'R² (Block Size) = {r_squared_blocksize:.4f}', \n",
    "             xy=(0.05, 0.90), \n",
    "             xycoords='axes fraction', \n",
    "             fontsize=10, \n",
    "             color=color2,\n",
    "             verticalalignment='top')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('atoms_vs_time_and_blocksize.png')\n",
    "\n",
    "# Print fitting details\n",
    "print(\"Mean Time Exponential Fitting Details:\")\n",
    "print(f\"Equation: y = {popt_time[0]:.4e} * exp({popt_time[1]:.4e} * x) + {popt_time[2]:.4f}\")\n",
    "print(f\"R-squared: {r_squared_time:.4f}\")\n",
    "\n",
    "print(\"\\nBlock Size Exponential Decay Fitting Details:\")\n",
    "print(f\"Equation: y = {popt_blocksize[0]:.4e} * exp(-{popt_blocksize[1]:.4e} * x) + {popt_blocksize[2]:.4f}\")\n",
    "print(f\"R-squared: {r_squared_blocksize:.4f}\")\n",
    "\n",
    "# Print data with predictions\n",
    "grouped_data['Predicted_Time'] = exp_func_time(grouped_data['Number_of_Atoms'], *popt_time)\n",
    "grouped_data['Time_Residual'] = grouped_data['Mean_Time'] - grouped_data['Predicted_Time']\n",
    "grouped_data['Predicted_BlockSize'] = exp_decay_func(grouped_data['Number_of_Atoms'], *popt_blocksize)\n",
    "grouped_data['BlockSize_Residual'] = grouped_data['Block_Size'] - grouped_data['Predicted_BlockSize']\n",
    "\n",
    "print(\"\\nData with Predictions:\")\n",
    "print(grouped_data[['PDB_File', 'Number_of_Atoms', 'Mean_Time', 'Predicted_Time', 'Time_Residual', \n",
    "                    'Block_Size', 'Predicted_BlockSize', 'BlockSize_Residual']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio_lib_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
